# -*- coding: utf-8 -*-
"""loader,spliters,embeding,with urdu doc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s627WY4qCPOHj9FZHvmxl1WINnT-xwbr

This note book is about urdu data preprocessing that how to load urdu data with multiples loader and then split in chunks to use multiple text spliters and emebiding create with huggingface multilangul emed models diffirent dimentions.
"""

pip install langchain

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-community beautifulsoup4

pip install requests

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-community

import nest_asyncio

nest_asyncio.apply()

from langchain_community.document_loaders.sitemap import SitemapLoader

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_community beautifulsoup4

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet "unstructured[all-docs]"

from langchain_community.document_loaders.image import UnstructuredImageLoader

loader = UnstructuredImageLoader("./example_data/layout-parser-paper-screenshot.png")

data = loader.load()

data[0]

"""# New Section

# New Section
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_community

pip install pdfminer.six

docs = loader.load()

pip install pypdf

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain_community pypdf

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader(
    "/content/Deep learning content_TXTToPDF.pdf",
)

docs

len(docs)

type(docs)



# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-text-splitters

from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter

chunk_size =200
chunk_overlap = 40

r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    # separators=["\n","\n\n"," ",""],
    chunk_overlap=chunk_overlap
)

c_splitter = CharacterTextSplitter(
    chunk_size=chunk_size,
    # separator="\n\n",
    chunk_overlap=chunk_overlap
)

docs

c_splitter.split_text(docs)

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet langchain-text-splitters tiktoken

from langchain_text_splitters import TokenTextSplitter

text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)

texts = text_splitter.split_text(docs)
print(texts[0])

pip install sentence_transformers

from langchain_text_splitters import SentenceTransformersTokenTextSplitter

splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)
text = "docs "

count_start_and_stop_tokens = 2
text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens
print(text_token_count)

# pip install nltk

from langchain_text_splitters import NLTKTextSplitter

text_splitter = NLTKTextSplitter(chunk_size=300)

texts = text_splitter.split_text(docs)
print(texts[0])

len(texts)

type(docs)

if isinstance(docs, list):
    docs = ' '.join([str(doc) for doc in docs])  # Adjust str(doc) to the appropriate method/attribute if needed

if isinstance(docs, list):
    docs = ' '.join([doc.text for doc in docs])  # Replace 'text' with the appropriate attribute

if isinstance(docs, list):
    # Ensure that all elements are converted to strings.
    docs = ' '.join([doc.text for doc in docs if doc is not None])  # Use the appropriate method or attribute for conversion

pip install nltk

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
print(word_tokenize("This is a test sentence."))

"""text charactor text splitters"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-text-splitters

from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="",
    chunk_size=512,
    chunk_overlap=20,
    length_function=len,
    is_separator_regex=False,
)
pages = text_splitter.split_text(docs)
print(pages[0])

len(pages)

print(pages[12])

"""recursivie text spliter    """

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=1000,
    chunk_overlap=100,
    length_function=len,
    is_separator_regex=False,
)
documents = text_splitter.split_text(docs)
print(documents[0])
print(documents[1])

len(documents)

print(documents[100])

"""embeding the documents"""

!pip install transformers langchain_huggingface

from langchain_huggingface import HuggingFaceEmbeddings

embed_model = HuggingFaceEmbeddings(model_name='Marqo/multilingual-e5-small')

embed = embed_model.embed_query("This is a test sentence.")



len(embed)

embed



embeddings = [embed_model.embed_query(page) for page in pages]

# Step 4: Print the results
for i, embed in enumerate(embeddings):
    print(f"Embedding for page {i + 1}: {embed}")

intfloat/multilingual-e5-base

embed_model_1 = HuggingFaceEmbeddings(model_name='intfloat/multilingual-e5-base')

embed = embed_model_1.embed_query("This is a test sentence.")

embeddings = [embed_model_1.embed_query(document) for document in documents]

# Step 4: Print the results
for i, embed in enumerate(embeddings):
    print(f"Embedding for page {i + 1}: {embed}")

len(embeddings)

embed

print(embeddings[15])

len(embed)